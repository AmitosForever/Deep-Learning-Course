{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzV9wsJ5pGhf"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/bubbles/50/000000/mind-map.png\" style=\"height:50px;display:inline\"> ECE 046211 - Technion - Deep Learning\n",
        "---\n",
        "\n",
        "## HW1 - Optimization and Automatic Differentiation\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq2c8X93pGhh"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/clouds/96/000000/keyboard.png\" style=\"height:50px;display:inline\"> Keyboard Shortcuts\n",
        "---\n",
        "* Run current cell: **Ctrl + Enter**\n",
        "* Run current cell and move to the next: **Shift + Enter**\n",
        "* Show lines in a code cell: **Esc + L**\n",
        "* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`\n",
        "* New cell below: **Esc + B**\n",
        "* Delete cell: **Esc + D, D** (two D's)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZZybn3NpGhh"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
        "---\n",
        "* Fill in\n",
        "\n",
        "|Name     |Campus Email| ID  |\n",
        "|---------|--------------------------------|----------|\n",
        "|Student 1| student_1@campus.technion.ac.il| 123456789|\n",
        "|Student 2| student_2@campus.technion.ac.il| 987654321|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDK5zqhdpGhi"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
        "---\n",
        "* Maximal garde: 100.\n",
        "* Submission only in **pairs**.\n",
        "    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).\n",
        "* **No handwritten submissions.** You can choose whether to answer in a Markdown cell in this notebook or attach a PDF with your answers.\n",
        "* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>\n",
        "* What you have to submit:\n",
        "    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ece046211_hw1_id1_id2.ipynb`.\n",
        "    * If you answered the questionss in a different file you should submit a `.zip` file with the name `ece046211_hw1_id1_id2.zip` with content:\n",
        "        * `ece046211_hw1_id1_id2.ipynb` - the code tasks\n",
        "        * `ece046211_hw1_id1_id2.pdf` - answers to questions.\n",
        "    * No other file-types (`.py`, `.docx`...) will be accepted.\n",
        "* Submission on the course website (Moodle).\n",
        "* **Latex in Colab** - in some cases, Latex equations may no be rendered. To avoid this, make sure to not use *bullets* in your answers (\"* some text here with Latex equations\" -> \"some text here with Latex equations\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmSj_UufpGhi"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/dusk/64/000000/online.png\" style=\"height:50px;display:inline\"> Working Online and Locally\n",
        "---\n",
        "* You can choose your working environment:\n",
        "    1. `Jupyter Notebook`, **locally** with <a href=\"https://www.anaconda.com/download\">Anaconda</a> or **online** on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
        "        * Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime`$\\rightarrow$ `Change Runtime Type` $\\rightarrow$`GPU`.\n",
        "    2. Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
        "        * Both allow editing and running Jupyter Notebooks.\n",
        "\n",
        "* Please refer to `Setting Up the Working Environment.pdf` on the Moodle or our GitHub (https://github.com/taldatech/ee046211-deep-learning) to help you get everything installed.\n",
        "* If you need any technical assistance, please go to our forum and describe your problem (preferably with images)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlp1Fp4ppGhj"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
        "---\n",
        "\n",
        "* [Part 1 - Theory](#-Part-1---Theory)\n",
        "    * [Q1 - Convergence of Gradient Descent](#-Question-1---Convergence-of-Gradient-Descent)\n",
        "    * [Q2 - Optimization and Gradient Descent](#-Question-2---Optimization-and-Gradient-Descent)\n",
        "    * [Q3 - Efficient Differentiation](#-Question-3---Efficient-Differentiation)\n",
        "    * [Q4 - Autodiff](#-Question-4----Automatic-Differentiation)\n",
        "    * [Q5 - Autodiff 2](#-Question-5----Automatic-Differentiation-2)\n",
        "* [Part 2 - Code Assignments](#-Part-2---Code-Assignments)\n",
        "    * [Task 1 - The Beale Function](#-Task-1---The-Beale-Function)\n",
        "    * [Task 2 - Building an Optimizer - Adam](#-Task-2---Building-an-Optimizer---Adam)\n",
        "    * [Task 3 - PyTorch Autograd](#-Task-3---PyTorch-Autograd)\n",
        "    * [Task 4 - Low Rank Matrix Factorization](#-Task-4---Low-Rank-Matrix-Factorization)\n",
        "* [Credits](#-Credits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKtSiQX_pGhj"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/cute-clipart/64/000000/ball-point-pen.png\" style=\"height:50px;display:inline\"> Part 1 - Theory\n",
        "---\n",
        "* You can choose whether to answser these straight in the notebook (Markdown + Latex) or use another editor (Word, LyX, Latex, Overleaf...) and submit an additional PDF file, **but no handwritten submissions**.\n",
        "* You can attach additional figures (drawings, graphs,...) in a separate PDF file, just make sure to refer to them in your answers.\n",
        "\n",
        "* $\\large\\LaTeX$ <a href=\"https://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index\">Cheat-Sheet</a> (to write equations)\n",
        "    * <a href=\"http://tug.ctan.org/info/latex-refsheet/LaTeX_RefSheet.pdf\">Another Cheat-Sheet</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsqSFZG1pGhj"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 1 - Convergence of Gradient Descent\n",
        "---\n",
        "Recall from the lecture notes:\n",
        "\n",
        "* **Definition**: A function $f$ is $\\beta$-smooth if: $$ \\forall w_1, w_2 \\in \\mathbb{R}^d: ||\\nabla f(w_1) - \\nabla f(w_2)|| \\leq \\beta ||w_1 -w_2|| $$\n",
        "* **Lemma**: If $f$ is $\\beta$-smooth then $$ f(w_1) -f(w_2) -\\nabla f(w_2)^T (w_1-w_2) \\leq \\frac{\\beta}{2} ||w_1-w_2||^2 $$\n",
        "\n",
        "Prove the lemma.\n",
        "\n",
        "Hints:\n",
        "* Represent $f$ as an integral: $f(x) − f(y) = \\int_0^1 \\nabla f(y + t(x-y))^T(x-y) dt $\n",
        "* Make use of Cauchy-Schwarz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "RBZnGj-AAAa5"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 2 - Optimization and Gradient Descent\n",
        "---\n",
        "\n",
        "We consider the **SGD (stochastic gradient descent)** model we saw in class, for $( w \\in \\mathbb{R} )$, with the quadratic cost function:\n",
        "\n",
        "$$\n",
        "f(w) = \\frac{1}{2} \\sum_{n=1}^{N} h_n w^2\n",
        "$$\n",
        "\n",
        "At each iteration $( t )$ of SGD, we randomly sample an index $( n(t) \\in {1, \\ldots, N} )$, and update according to\n",
        "\n",
        "$$\n",
        "w(t) = w(t - 1) - \\eta h_{n(t)} w(t - 1)\n",
        "$$\n",
        "\n",
        "where the random sampling is independent at each step. Here $( \\eta > 0 )$ is the learning rate.\n",
        "In addition, define\n",
        "\n",
        "$$\n",
        "h \\triangleq \\frac{1}{N} \\sum_{n=1}^{N} h_n , \\quad\n",
        "\\rho \\triangleq \\frac{1}{N} \\sum_{n=1}^{N} h_n^2 - h^2\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "1. Write the update equation for $\\mathbb{E}[w(t)]$ (that is, $\\mathbb{E}[w(t)]$ as a function of $\\mathbb{E}[w(t-1)]$, $h$, and $\\eta$).\n",
        "\n",
        "2. Find a condition on $\\eta$ (as a function of $h$) such that $\\mathbb{E}[w(t)]$ converges to the minimum at the maximal rate.\n",
        "\n",
        "3. What is the range of $\\eta$ values (an inequality as a function of $h$) for which $\\mathbb{E}[w(t)]$ necessarily converges to the minimum?\n",
        "\n",
        "4. Write the update equation for $\\mathbb{E}[w^2(t)]$ (that is, $\\mathbb{E}[w^2(t)]$ as a function of $\\mathbb{E}[w^2(t-1)]$, $h$, $\\rho$, and $\\eta$).\n",
        "\n",
        "5. Find a condition on $\\eta$ (as a function of $h$ and $\\rho$) such that $\\mathbb{E}[w^2(t)]$ converges to the minimum at the maximal rate.\n",
        "\n",
        "6. What is the range of $\\eta$ values (an inequality as a function of $h$ and $\\rho$) for which $\\mathbb{E}[w^2(t)]$ necessarily converges to the minimum?\n",
        "\n",
        "7. What is the range of $\\eta$ values (an inequality as a function of $h$ and $\\rho$) for which $\\mathbb{E}[f(w(t))]$ necessarily converges to the minimum?\n",
        "\n",
        "8. Write the update equation for $\\log(w(t))$, and use it to prove that, with probability 1,\n",
        "$$\n",
        "\\lim_{t \\rightarrow \\infty} \\frac{1}{t} \\log(w(t)) = q(\\eta) \\triangleq \\frac{1}{N} \\sum_{n=1}^{N} \\log(1 - \\eta h_n)\n",
        "$$\n",
        "\n",
        "*Hint:* Use the Law of Large Numbers, i.e., if we have i.i.d. samples ${X_n}*{n=1}^{N}$ drawn from a distribution with mean $\\mathbb{E}[X]$ and bounded variance, then\n",
        "$$\n",
        "\\lim*{N \\rightarrow \\infty} \\frac{1}{N} \\sum_{n=1}^{N} X_n = \\mathbb{E}[X]\n",
        "$$\n",
        "with probability 1.\n",
        "\n",
        "9. What is the condition that must hold on $q(\\eta)$ such that $w(t)$ converges to 0 with probability 1?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "FWT5R18DAAa5"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 3 - Efficient Differentiation\n",
        "---\n",
        "We wish to optimize a loss function $\\mathcal{L}\\left(\\mathbf{w}\\right)$ for  $\\mathbf{w}\\in\\mathbb{R}^{d}$ using Gradient Descent (GD) with some step size schedule $\\eta_{t}$\n",
        "\\begin{equation}\n",
        "(1)\\:\\: \\forall t=1,2,..:\\mathbf{w}\\left(t\\right)=\\mathbf{w}\\left(t-1\\right)-\\eta_{t}\\nabla\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)\n",
        "\\end{equation}\n",
        "initialized from some $\\mathbf{w}\\left(0\\right)$.\n",
        "We would like to learn the best step size schedule using GD. **Hint**: throughout this question, you should use the *chain rule*.\n",
        "\n",
        "1. Suppose we can consider each $\\eta_{t}$ as a separate parameter for each $t$. We initialize this parameter with $\\eta_{0}$ and update $\\eta_{t-1}$ with a GD step on $\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)$\n",
        "\\begin{equation}\n",
        "(2)\\:\\:\\eta_{t}=\\eta_{t-1}-\\alpha_{t}\\frac{\\partial\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)}{\\partial\\eta_{t-1}}\n",
        "\\end{equation} for every step of eq. $(1)$, where $\\alpha_{t}$ is the another step size. Calculate $\\partial\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)/\\partial\\eta_{t-1}$ as a function of the loss gradients $\\nabla\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)$ and $\\nabla\\mathcal{L}\\left(\\mathbf{w}\\left(t-2\\right)\\right)$.\n",
        "2. Now suppose we want to similarly update $\\alpha_{t-1}$ using GD step on $\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)$ every step of eq. $(2)$ with update step $\\kappa_{t}$\n",
        "\\begin{equation}\n",
        "\\alpha_{t}=\\alpha_{t-1}-\\kappa_{t}\\frac{\\partial\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)}{\\partial\\alpha_{t-1}}\\,.\n",
        "\\end{equation} Calculate $\\partial\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)/\\partial\\alpha_{t-1}$ as a function of $\\left\\{ \\nabla\\mathcal{L}\\left(\\mathbf{w}\\left(t-k\\right)\\right)\\right\\} _{k=1}^{3}$.\n",
        "3. Now we wish to update $\\left(\\eta_{t-1},\\eta_{t-2}\\right)$ by doing a GD step on $\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)$\n",
        "\\begin{equation}\n",
        "(3)\\:\\:\\left(\\eta_{t+1},\\eta_{t}\\right)=\\left(\\eta_{t-1},\\eta_{t-2}\\right)-\\alpha_{t}\\left(\\frac{\\partial\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)}{\\partial\\eta_{t-1}},\\frac{\\partial\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)}{\\partial\\eta_{t-2}}\\right)\n",
        "\\end{equation} every two steps of eq. $(1)$. Calculate the derivative $\\frac{\\partial\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)}{\\partial\\eta_{t-2}}$  as a function of $\\eta_{t-1}$, $\\left\\{ \\nabla\\mathcal{L}\\left(\\mathbf{w}\\left(t-k\\right)\\right)\\right\\} _{k=1}^{3}$, and $\\nabla^{2}\\mathcal{L}\\left(\\mathbf{w}\\left(t-2\\right)\\right)$.\n",
        "4. Now we wish again to update $\\left(\\eta_{t},\\eta_{t+1},...,\\eta_{t+T}\\right)$ by doing a GD step on $\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)$ every $T$ steps of eq. $(1)$\n",
        "\\begin{equation}\n",
        "(4)\\:\\:\\left(\\eta_{t+T},...,\\eta_{t}\\right)=\\left(\\eta_{t-1},...,\\eta_{t-1-T}\\right)-\\alpha_{t}\\left(\\frac{\\partial\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)}{\\partial\\eta_{t-1}},...,\\frac{\\partial\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)}{\\partial\\eta_{t-1-T}}\\right)\n",
        "\\end{equation} Calculate the derivative $\\frac{\\partial\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)}{\\partial\\eta_{t-\\tau}}$ as a function of $\\left\\{ \\eta_{t-k},\\nabla^{2}\\mathcal{L}\\left(\\mathbf{w}\\left(t-k-1\\right)\\right)\\right\\} _{k=1}^{\\tau-1}$, $\\nabla\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)$ and $\\nabla\\mathcal{L}\\left(\\mathbf{w}\\left(t-\\tau-1\\right)\\right)$.\n",
        "5. Compare this approach (eq. $(4)$ with $T>1$) to the first one (eq. $(2)$). Name one advantage for each approach.\n",
        " Hints: Think of computional complexity, ease of optimization, suitability\n",
        " of the objective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVF9OJWWpGhl"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 4 -  Automatic Differentiation\n",
        "---\n",
        "\n",
        "Consider the scalar function: $$ f = \\exp(\\exp(x) + \\exp(x)^2) +\\sin(\\exp(x) + \\exp(x)^2) $$\n",
        "\n",
        "1. Write down the derivative w.r.t. $x$ explicitly, i.e., $\\frac{d f}{d x}$\n",
        "2. We define the following intermediate variables: $$ a = \\exp(x) $$ $$ b=a^2 $$ $$ c = a+b $$ $$ d=\\exp(c) $$ $$ e=\\sin(c) $$ $$ f=d+e $$ Draw a graph picturing the relationship between all variables (called the **computation graph**).\n",
        "3. Using the graph, write down the derivatives of the individual terms, working backwards to compute the derivative of $f$ (i.e., write down the derivatives $\\frac{df}{dd}, \\frac{df}{de}, ..., \\frac{df}{dx}$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7D9SSTRAAa6"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 5 -  Automatic Differentiation 2\n",
        "---\n",
        "Write down the chain rule in the dual numbers representation for the following: $$ f(g(h(x + \\epsilon x'))) $$ What is $ \\frac{df(x)}{dx} $?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D-14iM7pGhm"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/officel/80/000000/code.png\" style=\"height:50px;display:inline\"> Part 2 - Code Assignments\n",
        "---\n",
        "* You must write your code in this notebook and save it with the output of aall of the code cells.\n",
        "* Additional text can be added in Markdown cells.\n",
        "* You can use any other IDE you like (PyCharm, VSCode...) to write/debug your code, but for the submission you must copy it to this notebook, run the code and save the notebook with the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bya0qUGYpGhm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77cd53b2-2faa-49d3-d501-235896ad9f32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current cuda device:  0\n",
            "running calculations on device:  cuda:0\n"
          ]
        }
      ],
      "source": [
        "# imports for the practice (you can add more if you need)\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.colors import LogNorm\n",
        "from sklearn.datasets import load_iris\n",
        "seed = 211\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "# %matplotlib notebook\n",
        "%matplotlib inline\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"current cuda device: \", torch.cuda.current_device())\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"running calculations on device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HC-UE1cNpGhn"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 1 - The Beale Function\n",
        "---\n",
        "The Beale function is defined as follows: $$ f(x, y) = (1.5 - x + xy)^{2} + (2.25 - x + xy^{2})^{2} + (2.625 - x +xy^{3})^{2}$$\n",
        "\n",
        "1. What is the global minima of this function?\n",
        "2. Implement the Beale function: `beale_f(x,y)`.\n",
        "3. Implement a function, `beale_grads(x,y)` that returns the gradients of the Beale function.\n",
        "4. 3D plot the Beale function wit the global minima you found. Use Matplotlib's `ax.plot_surface(x_mesh, y_mesh, z, norm=LogNorm(), rstride=1, cstride=1, edgecolor='none', alpha=.8, cmap=plt.cm.jet)` for the function, and `ax.plot(x, y, f(x, y), 'r*', markersize=20)` for the minima.\n",
        "4. 2D plot the contours with `ax.contour(x_mesh, y_mesh, z, levels=np.logspace(-.5, 5, 35), norm=LogNorm(), cmap=plt.cm.jet)` and the minima with `ax.plot(x, y, 'r*', markersize=20)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsHtDWllpGhn"
      },
      "source": [
        "Your Answers Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukJi9-rapGhn"
      },
      "outputs": [],
      "source": [
        "# Set the manually calculated minima\n",
        "min_x = None\n",
        "min_y = None\n",
        "\n",
        "def beale_f(x, y):\n",
        "    value = None\n",
        "    \"\"\"\n",
        "    Your Code Here\n",
        "    \"\"\"\n",
        "    return value\n",
        "\n",
        "def beale_grads(x, y):\n",
        "    dx, dy = None, None\n",
        "    \"\"\"\n",
        "    Your Code Here\n",
        "    \"\"\"\n",
        "\n",
        "    grads = np.array([dx, dy])\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IepBb4_9pGho"
      },
      "outputs": [],
      "source": [
        "minima = np.array([min_x, min_y])\n",
        "beale_res = beale_f(*minima)\n",
        "grads_res = beale_grads(*minima)\n",
        "print(f\"minima (1x2 row vector shape): {minima}\")\n",
        "print(f\"beale_f output: {beale_res}\")\n",
        "print(f\"beale_grad output: {grads_res}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9gTdH7oAAa8"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 2 - Building an Optimizer - Adam\n",
        "---\n",
        "In this task, you are going to implement the Adam optimizer. We are giving the skeleton of the code and the description of the methods, and you need to implement the optimizer.\n",
        "\n",
        "Recall the Adam update rule:\n",
        "$$ m_{k+1} = \\beta_1 m_k + (1-\\beta_1)\\nabla f(w^k) = \\beta_1 m_k + (1-\\beta_1)g_k $$  $$ v_{k+1} = \\beta_2 v_k + (1-\\beta_2)(\\nabla f(w^k))^2 = \\beta_2 v_k + (1-\\beta_2)g^2_k $$ Then, they use an **unbiased** estimation: $$ \\hat{m}_{k+1} = \\frac{m_{k+1}}{1 -\\beta_1^{k+1}} $$ $$ \\hat{v}_{k+1} = \\frac{v_{k+1}}{1 -\\beta_2^{k+1}} $$ (the $\\beta$'s are taken with the power of the current iteration) $$ w_{k+1} = w_k -\\frac{\\alpha}{\\sqrt{\\hat{v}_{k+1}} +\\epsilon}\\hat{m}_{k+1} $$\n",
        "\n",
        "* $\\epsilon$ deafult's is $10^{-8}$\n",
        "\n",
        "\n",
        "1. Implement `class AdamOptimizer()`.\n",
        "    * `function` is the Python function you want to optimize.\n",
        "    * `gradients` is the Python function that returns the gradients of `function`.\n",
        "    * `x_init` and `y_init` are the initialization points for the optimizer.\n",
        "    * Save the `path` of the optimizer (the minima points the optimizer visits during the optimization).\n",
        "    * Stopping criterion: change in minima `<1e-7`.\n",
        "    * **You can change the class however you wish, you can remove/add variables and methods as you wish**\n",
        "2. For ` x_init=0.7, y_init=1.4, learning_rate=0.1, beta1=0.9, beta2=0.999`, optimize the Beale function. Plot the results **with the path taken** (better do it on the 2D contour plot).\n",
        "3. Choose different initialization and learning rate and show the results as in 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-otg5rj2AAa8"
      },
      "outputs": [],
      "source": [
        "class AdamOptimizer():\n",
        "    def __init__(self, function, gradients, x_init=None, y_init=None,\n",
        "                 learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.f = function\n",
        "        self.g = gradients\n",
        "        scale = 3.0\n",
        "        self.current_val = np.zeros([2])\n",
        "        if x_init is not None:\n",
        "            self.current_val[0] = x_init\n",
        "        else:\n",
        "            self.current_val[0] = np.random.uniform(low=-scale, high=scale)\n",
        "        if y_init is not None:\n",
        "            self.current_val[1] = y_init\n",
        "        else:\n",
        "            self.current_val[1] = np.random.uniform(low=-scale, high=scale)\n",
        "        print(\"x_init: {:.3f}\".format(self.current_val[0]))\n",
        "        print(\"y_init: {:.3f}\".format(self.current_val[1]))\n",
        "\n",
        "        self.lr = learning_rate\n",
        "        self.grads_first_moment = np.zeros([2])\n",
        "        self.grads_second_moment = np.zeros([2])\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # for accumulation of loss and path (w, b)\n",
        "        self.z_history = []\n",
        "        self.x_history = []\n",
        "        self.y_history = []\n",
        "\n",
        "\n",
        "    def func(self, variables):\n",
        "        \"\"\"Beale function.\n",
        "\n",
        "        Args:\n",
        "          variables: input data, shape: 1-rank Tensor (vector) np.array\n",
        "            x: x-dimension of inputs\n",
        "            y: y-dimension of inputs\n",
        "\n",
        "        Returns:\n",
        "          z: Beale function value at (x, y)\n",
        "        \"\"\"\n",
        "\n",
        "    def gradients(self, variables):\n",
        "        \"\"\"Gradient of Beale function.\n",
        "\n",
        "        Args:\n",
        "          variables: input data, shape: 1-rank Tensor (vector) np.array\n",
        "            x: x-dimension of inputs\n",
        "            y: y-dimension of inputs\n",
        "\n",
        "        Returns:\n",
        "          grads: [dx, dy], shape: 1-rank Tensor (vector) np.array\n",
        "            dx: gradient of Beale function with respect to x-dimension of inputs\n",
        "            dy: gradient of Beale function with respect to y-dimension of inputs\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    def weights_update(self, grads, time):\n",
        "        \"\"\"Weights update using Adam.\n",
        "\n",
        "          g1 = beta1 * g1 + (1 - beta1) * grads\n",
        "          g2 = beta2 * g2 + (1 - beta2) * grads ** 2\n",
        "          g1_unbiased = g1 / (1 - beta1**time)\n",
        "          g2_unbiased = g2 / (1 - beta2**time)\n",
        "          w = w - lr * g1_unbiased / (sqrt(g2_unbiased) + epsilon)\n",
        "        \"\"\"\n",
        "\n",
        "    def history_update(self, z, x, y):\n",
        "        \"\"\"Accumulate all interesting variables\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    def train(self, max_steps):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKouk-J0pGhq"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Your Code Here\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3JexsWqAAa9"
      },
      "outputs": [],
      "source": [
        "opt = AdamOptimizer(beale_f, beale_grads, x_init=0.7, y_init=1.4, learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Tug7fZS8AAa9"
      },
      "outputs": [],
      "source": [
        "%time\n",
        "opt.train(1000)\n",
        "print(\"Global minima\")\n",
        "print(\"x*: {:.2f}  y*: {:.2f}\".format(minima[0], minima[1]))\n",
        "print(\"Solution using the gradient descent\")\n",
        "print(\"x: {:.4f}  y: {:.4f}\".format(opt.x, opt.y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HYAAPl6AAa9"
      },
      "outputs": [],
      "source": [
        "# plot the Beale function values during the optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OO60u5uhAAa9"
      },
      "outputs": [],
      "source": [
        "# plot the optimization path\n",
        "path = opt.path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGSxoUKppGhs"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 3 - PyTorch Autograd\n",
        "---\n",
        "For the function from the theory practice: $$ f = \\exp(\\exp(x) + \\exp(x)^2) +\\sin(\\exp(x) + \\exp(x)^2)  $$\n",
        "\n",
        "1. Implement it and its dervative (explicitly) using `torch`.\n",
        "2. Define a scalar tensor `x` and use `autograd` to calculate the derivative w.r.t $x$. Does the result correspond to the output of the function the calculates the derivative explicitly?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qopWh-QupGhs"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "    f_val = None\n",
        "    a = torch.exp(x)\n",
        "    b = a**2\n",
        "    c = a + b\n",
        "    f_val = torch.exp(c) + torch.sin(c)\n",
        "    return f_val\n",
        "\n",
        "def derv_f(x):\n",
        "    derv_val = None\n",
        "    a = torch.exp(x)\n",
        "    b = a**2\n",
        "    c = a + b\n",
        "    d = torch.cos(c) + torch.exp(c)\n",
        "    e = a * (1 + 2*a)\n",
        "    derv_val = e * d\n",
        "    return derv_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oNO19wRspGhs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "391b6559-fb59-464d-e6eb-69aae583983e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.5000, requires_grad=True)\n",
            "tensor(555.9719)\n",
            "tensor(555.9719)\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(0.5, requires_grad=True)\n",
        "print(x)\n",
        "f_res = f(x)\n",
        "f_manual_grad = derv_f(x.detach())\n",
        "\n",
        "f_res.backward()\n",
        "# Calculate with torch autograd\n",
        "f_autograd = x.grad\n",
        "\n",
        "\n",
        "print(f_manual_grad)\n",
        "print(f_autograd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrof_SsJpGht"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 4 - Low Rank Matrix Factorization\n",
        "---\n",
        "Consider the following optimization problem: $$ \\min_{\\hat{U}, \\hat{V}}||A - \\hat{U}\\hat{V}||_F^{2} $$ Where $A \\in \\mathcal{R}^{m \\times n},\\hat{U} \\in \\mathcal{R}^{m \\times r}, \\hat{V} \\in \\mathcal{R}^{r \\times n} $ and $r < min(m,n)$ ($r$ is the rank of the matrix). $||\\cdot||_F^2$ denotes the Frobenius norm.\n",
        "\n",
        "1. Implement a function, `gd_factorize_ad(A, rank, num_epochs=1000, lr=0.01)`, that given a 2D tensor `A` and a `rank`, will calculate the low-rank factorization of `A` using **gradient decsent**. Compute and apply all the gradients of $\\hat{U}$ and of $\\hat{V}$ once per epoch. $\\hat{U}$ and $\\hat{V}$ should be initially created with uniform random values. Use PyTorch's `autograd` for the gradients.\n",
        "    * To compute the squared Frobenius norm loss (reconstruction loss), use `torch.nn.functional.mse loss with reduction=’sum’`.\n",
        "\n",
        "2. Use the provided `data` of the Iris dataset of 150 instances and 4 features. Apply `gd_factorize_ad` to compute the 2-rank matrix factorization of `data`. What is the reconstruction loss?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HCFYO3QBpGht"
      },
      "outputs": [],
      "source": [
        "df = load_iris(as_frame=True).data # option 1\n",
        "# df = pd.read_csv('./iris.data', header=None) # option 2\n",
        "data = torch.tensor(df.iloc[:, [0, 1, 2, 3]].values)\n",
        "data = data - data.mean(dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sLIgji02pGht"
      },
      "outputs": [],
      "source": [
        "def gd_factorize_ad(A, rank, num_epochs=1000, lr=0.01):\n",
        "    # initialize\n",
        "    U = None\n",
        "    V = None\n",
        "\n",
        "    U = torch.rand(A.shape[0], rank, requires_grad=True)\n",
        "    V = torch.rand(rank, A.shape[1], requires_grad=True)\n",
        "\n",
        "    # implement gradient descent\n",
        "    for epoch in range(num_epochs):\n",
        "        loss = torch.nn.functional.mse_loss(A, U@V, reduction='sum')\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            U -= lr * U.grad\n",
        "            V -= lr * V.grad\n",
        "        U.grad.zero_()\n",
        "        V.grad.zero_()\n",
        "        if epoch % 5 == 0:\n",
        "            print(f'epoch: {epoch}, loss: {loss}')\n",
        "    return U, V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZmImbLvIpGht",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a972833-db5a-4303-dd94-6c1a20612e3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, loss: 888.7951049804688\n",
            "epoch: 5, loss: 531.0764770507812\n",
            "epoch: 10, loss: 72.64857482910156\n",
            "epoch: 15, loss: 51.544254302978516\n",
            "epoch: 20, loss: 51.36110305786133\n",
            "epoch: 25, loss: 51.35871124267578\n",
            "epoch: 30, loss: 51.35849380493164\n",
            "epoch: 35, loss: 51.358375549316406\n",
            "epoch: 40, loss: 51.35826110839844\n",
            "epoch: 45, loss: 51.35813522338867\n",
            "epoch: 50, loss: 51.358001708984375\n",
            "epoch: 55, loss: 51.35786437988281\n",
            "epoch: 60, loss: 51.35771179199219\n",
            "epoch: 65, loss: 51.35755157470703\n",
            "epoch: 70, loss: 51.35737609863281\n",
            "epoch: 75, loss: 51.357181549072266\n",
            "epoch: 80, loss: 51.35698318481445\n",
            "epoch: 85, loss: 51.35675811767578\n",
            "epoch: 90, loss: 51.35652160644531\n",
            "epoch: 95, loss: 51.356258392333984\n",
            "epoch: 100, loss: 51.35597610473633\n",
            "epoch: 105, loss: 51.35566711425781\n",
            "epoch: 110, loss: 51.355323791503906\n",
            "epoch: 115, loss: 51.35496520996094\n",
            "epoch: 120, loss: 51.35456848144531\n",
            "epoch: 125, loss: 51.35413360595703\n",
            "epoch: 130, loss: 51.35365295410156\n",
            "epoch: 135, loss: 51.3531379699707\n",
            "epoch: 140, loss: 51.352561950683594\n",
            "epoch: 145, loss: 51.3519401550293\n",
            "epoch: 150, loss: 51.351253509521484\n",
            "epoch: 155, loss: 51.350502014160156\n",
            "epoch: 160, loss: 51.34967803955078\n",
            "epoch: 165, loss: 51.348777770996094\n",
            "epoch: 170, loss: 51.347782135009766\n",
            "epoch: 175, loss: 51.34668731689453\n",
            "epoch: 180, loss: 51.34549331665039\n",
            "epoch: 185, loss: 51.34416961669922\n",
            "epoch: 190, loss: 51.34272003173828\n",
            "epoch: 195, loss: 51.34111785888672\n",
            "epoch: 200, loss: 51.33935546875\n",
            "epoch: 205, loss: 51.3374137878418\n",
            "epoch: 210, loss: 51.33527755737305\n",
            "epoch: 215, loss: 51.33292007446289\n",
            "epoch: 220, loss: 51.330322265625\n",
            "epoch: 225, loss: 51.32746124267578\n",
            "epoch: 230, loss: 51.32429885864258\n",
            "epoch: 235, loss: 51.3208122253418\n",
            "epoch: 240, loss: 51.31696701049805\n",
            "epoch: 245, loss: 51.31272888183594\n",
            "epoch: 250, loss: 51.308048248291016\n",
            "epoch: 255, loss: 51.30287551879883\n",
            "epoch: 260, loss: 51.29716873168945\n",
            "epoch: 265, loss: 51.29087829589844\n",
            "epoch: 270, loss: 51.2839241027832\n",
            "epoch: 275, loss: 51.27625274658203\n",
            "epoch: 280, loss: 51.26777648925781\n",
            "epoch: 285, loss: 51.25841522216797\n",
            "epoch: 290, loss: 51.24809265136719\n",
            "epoch: 295, loss: 51.23667907714844\n",
            "epoch: 300, loss: 51.22407531738281\n",
            "epoch: 305, loss: 51.21015930175781\n",
            "epoch: 310, loss: 51.194801330566406\n",
            "epoch: 315, loss: 51.177833557128906\n",
            "epoch: 320, loss: 51.15909957885742\n",
            "epoch: 325, loss: 51.13841247558594\n",
            "epoch: 330, loss: 51.11557388305664\n",
            "epoch: 335, loss: 51.090370178222656\n",
            "epoch: 340, loss: 51.06254196166992\n",
            "epoch: 345, loss: 51.031829833984375\n",
            "epoch: 350, loss: 50.9979248046875\n",
            "epoch: 355, loss: 50.960533142089844\n",
            "epoch: 360, loss: 50.919273376464844\n",
            "epoch: 365, loss: 50.87376022338867\n",
            "epoch: 370, loss: 50.823577880859375\n",
            "epoch: 375, loss: 50.768253326416016\n",
            "epoch: 380, loss: 50.707275390625\n",
            "epoch: 385, loss: 50.64009475708008\n",
            "epoch: 390, loss: 50.566104888916016\n",
            "epoch: 395, loss: 50.484642028808594\n",
            "epoch: 400, loss: 50.39499282836914\n",
            "epoch: 405, loss: 50.29638671875\n",
            "epoch: 410, loss: 50.187984466552734\n",
            "epoch: 415, loss: 50.06888198852539\n",
            "epoch: 420, loss: 49.938106536865234\n",
            "epoch: 425, loss: 49.794620513916016\n",
            "epoch: 430, loss: 49.63731384277344\n",
            "epoch: 435, loss: 49.46499252319336\n",
            "epoch: 440, loss: 49.27641677856445\n",
            "epoch: 445, loss: 49.07025909423828\n",
            "epoch: 450, loss: 48.84514617919922\n",
            "epoch: 455, loss: 48.599632263183594\n",
            "epoch: 460, loss: 48.33222961425781\n",
            "epoch: 465, loss: 48.04142379760742\n",
            "epoch: 470, loss: 47.72567367553711\n",
            "epoch: 475, loss: 47.38343811035156\n",
            "epoch: 480, loss: 47.01319885253906\n",
            "epoch: 485, loss: 46.613487243652344\n",
            "epoch: 490, loss: 46.18292236328125\n",
            "epoch: 495, loss: 45.720211029052734\n",
            "epoch: 500, loss: 45.2242431640625\n",
            "epoch: 505, loss: 44.694087982177734\n",
            "epoch: 510, loss: 44.12905502319336\n",
            "epoch: 515, loss: 43.52873611450195\n",
            "epoch: 520, loss: 42.89305877685547\n",
            "epoch: 525, loss: 42.222312927246094\n",
            "epoch: 530, loss: 41.51719284057617\n",
            "epoch: 535, loss: 40.77885437011719\n",
            "epoch: 540, loss: 40.00889205932617\n",
            "epoch: 545, loss: 39.20938491821289\n",
            "epoch: 550, loss: 38.382896423339844\n",
            "epoch: 555, loss: 37.53242874145508\n",
            "epoch: 560, loss: 36.66143798828125\n",
            "epoch: 565, loss: 35.773738861083984\n",
            "epoch: 570, loss: 34.87348175048828\n",
            "epoch: 575, loss: 33.965057373046875\n",
            "epoch: 580, loss: 33.05302810668945\n",
            "epoch: 585, loss: 32.14203643798828\n",
            "epoch: 590, loss: 31.2366886138916\n",
            "epoch: 595, loss: 30.341495513916016\n",
            "epoch: 600, loss: 29.460758209228516\n",
            "epoch: 605, loss: 28.598499298095703\n",
            "epoch: 610, loss: 27.758392333984375\n",
            "epoch: 615, loss: 26.943706512451172\n",
            "epoch: 620, loss: 26.157268524169922\n",
            "epoch: 625, loss: 25.401432037353516\n",
            "epoch: 630, loss: 24.67807960510254\n",
            "epoch: 635, loss: 23.98861312866211\n",
            "epoch: 640, loss: 23.3339786529541\n",
            "epoch: 645, loss: 22.714698791503906\n",
            "epoch: 650, loss: 22.13089370727539\n",
            "epoch: 655, loss: 21.58232879638672\n",
            "epoch: 660, loss: 21.068464279174805\n",
            "epoch: 665, loss: 20.58849334716797\n",
            "epoch: 670, loss: 20.141380310058594\n",
            "epoch: 675, loss: 19.725927352905273\n",
            "epoch: 680, loss: 19.340791702270508\n",
            "epoch: 685, loss: 18.98453140258789\n",
            "epoch: 690, loss: 18.6556396484375\n",
            "epoch: 695, loss: 18.352577209472656\n",
            "epoch: 700, loss: 18.073787689208984\n",
            "epoch: 705, loss: 17.817731857299805\n",
            "epoch: 710, loss: 17.58289337158203\n",
            "epoch: 715, loss: 17.367795944213867\n",
            "epoch: 720, loss: 17.1710205078125\n",
            "epoch: 725, loss: 16.9912052154541\n",
            "epoch: 730, loss: 16.82705307006836\n",
            "epoch: 735, loss: 16.677339553833008\n",
            "epoch: 740, loss: 16.54090690612793\n",
            "epoch: 745, loss: 16.416675567626953\n",
            "epoch: 750, loss: 16.30362892150879\n",
            "epoch: 755, loss: 16.200828552246094\n",
            "epoch: 760, loss: 16.10740089416504\n",
            "epoch: 765, loss: 16.02252960205078\n",
            "epoch: 770, loss: 15.945474624633789\n",
            "epoch: 775, loss: 15.875544548034668\n",
            "epoch: 780, loss: 15.812103271484375\n",
            "epoch: 785, loss: 15.754569053649902\n",
            "epoch: 790, loss: 15.702409744262695\n",
            "epoch: 795, loss: 15.65513801574707\n",
            "epoch: 800, loss: 15.612305641174316\n",
            "epoch: 805, loss: 15.573505401611328\n",
            "epoch: 810, loss: 15.538363456726074\n",
            "epoch: 815, loss: 15.506546020507812\n",
            "epoch: 820, loss: 15.477738380432129\n",
            "epoch: 825, loss: 15.45166301727295\n",
            "epoch: 830, loss: 15.428062438964844\n",
            "epoch: 835, loss: 15.406704902648926\n",
            "epoch: 840, loss: 15.387378692626953\n",
            "epoch: 845, loss: 15.369895935058594\n",
            "epoch: 850, loss: 15.354080200195312\n",
            "epoch: 855, loss: 15.339771270751953\n",
            "epoch: 860, loss: 15.32682991027832\n",
            "epoch: 865, loss: 15.31512451171875\n",
            "epoch: 870, loss: 15.30453872680664\n",
            "epoch: 875, loss: 15.294964790344238\n",
            "epoch: 880, loss: 15.286308288574219\n",
            "epoch: 885, loss: 15.278478622436523\n",
            "epoch: 890, loss: 15.27139949798584\n",
            "epoch: 895, loss: 15.264999389648438\n",
            "epoch: 900, loss: 15.259210586547852\n",
            "epoch: 905, loss: 15.253978729248047\n",
            "epoch: 910, loss: 15.249248504638672\n",
            "epoch: 915, loss: 15.244970321655273\n",
            "epoch: 920, loss: 15.241101264953613\n",
            "epoch: 925, loss: 15.237605094909668\n",
            "epoch: 930, loss: 15.234443664550781\n",
            "epoch: 935, loss: 15.231588363647461\n",
            "epoch: 940, loss: 15.22900390625\n",
            "epoch: 945, loss: 15.226669311523438\n",
            "epoch: 950, loss: 15.224555969238281\n",
            "epoch: 955, loss: 15.222647666931152\n",
            "epoch: 960, loss: 15.220921516418457\n",
            "epoch: 965, loss: 15.219361305236816\n",
            "epoch: 970, loss: 15.217950820922852\n",
            "epoch: 975, loss: 15.216675758361816\n",
            "epoch: 980, loss: 15.215521812438965\n",
            "epoch: 985, loss: 15.21448040008545\n",
            "epoch: 990, loss: 15.213537216186523\n",
            "epoch: 995, loss: 15.212685585021973\n"
          ]
        }
      ],
      "source": [
        "U, V = gd_factorize_ad(data.float(), rank=2, num_epochs=1000, lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgZUKV2NpGhu"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
        "---\n",
        "* Icons made by <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a>\n",
        "* Icons from <a href=\"https://icons8.com/\">Icons8.com</a> - https://icons8.com\n",
        "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ee046211_hw1_optimization_autograd_sol.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}